{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib as mp\n",
    "import time\n",
    "import seaborn as sb\n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections of the report \n",
    "\n",
    "-\tAbstract\n",
    "-\tIntroduction\n",
    "-\tMethods\n",
    "    -\tCleaning the data and creating new input features\n",
    "    - Analysing and visualising the data\n",
    "    - Preparing the inputs and choosing suitable features\n",
    "    - Selecting and training a model\n",
    "- Training the model\n",
    "- K fold cross validation\n",
    "- Fine-tuning the models\n",
    "-\tEvaluation\n",
    "-\tDiscussion \n",
    "-\tConclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This study will be taking a data driven approach towards classifying images of seals. The dataset used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Method\n",
    "\n",
    "### 2.1 Loading and cleaning the data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "\n",
    "#binary datasets\n",
    "X_train_binary = pd.read_csv('/data/CS5014-P2/binary/X_train.csv', header=None)\n",
    "Y_train_binary = pd.read_csv('/data/CS5014-P2/binary/Y_train.csv', header=None)\n",
    "X_test_binary = pd.read_csv('/data/CS5014-P2/binary/X_test.csv', header=None)\n",
    "\n",
    "#multi datasets\n",
    "X_train_multi = pd.read_csv('/data/CS5014-P2/multi/X_train.csv', header=None)\n",
    "Y_train_multi = pd.read_csv('/data/CS5014-P2/multi/Y_train.csv', header=None)\n",
    "X_test_multi = pd.read_csv('/data/CS5014-P2/multi/X_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) **Check for Null-values and inconsistent data types:** Upon looking at that dataset, we can see that each column has an equal amount of non-null values, indicating that there are **no instances** of missing data. Moreover, all the data types are float64, indicating a consistency in data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62210 entries, 0 to 62209\n",
      "Columns: 964 entries, 0 to 963\n",
      "dtypes: float64(964)\n",
      "memory usage: 457.5 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62210 entries, 0 to 62209\n",
      "Columns: 964 entries, 0 to 963\n",
      "dtypes: float64(964)\n",
      "memory usage: 457.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check for Null-values\n",
    "print(X_train_multi.info(null_counts=True))\n",
    "print(X_train_binary.info(null_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) **Check for duplicate values**: No duplicate values were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "62208\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# remove duplicate rows\n",
    "datasets = [X_train_binary,Y_train_binary, X_test_binary, X_train_multi, X_train_multi, X_test_multi]\n",
    "\n",
    "for data in datasets:\n",
    "    duplicate_rows = data[data.duplicated()]\n",
    "    print(len(duplicate_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No null values, no duplicate and all the data types were for the dtype float64. Nothing needed to change so will proceed to visualising the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Create a preliminary test set\n",
    "\n",
    "Despite there being a already a test set in place, a preliminary test set was created, to evaluate the performance of the model and to see how well the model could\n",
    "generalise on unseen data. The training data was split with an 80/20 split in stratified fashion, in order to keep the proportion of the classes as the dataset\n",
    "was already heavily imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the training set\n",
    "X_train_B, X_pre_test_B, y_train_B, y_pre_test_B = train_test_split(X_train_binary, Y_train_binary, test_size=0.20, stratify=Y_train_binary, random_state=27)\n",
    "X_train_M, X_pre_test_M, y_train_M, y_pre_test_M = train_test_split(X_train_multi, Y_train_multi, test_size=0.20, stratify=Y_train_multi, random_state=27)\n",
    "\n",
    "# Resets the indexes\n",
    "X_train_B = X_train_B.reset_index(drop=True)\n",
    "X_pre_test_B = X_pre_test_B.reset_index(drop=True)\n",
    "y_train_B = y_train_B.reset_index(drop=True)\n",
    "y_pre_test_B = y_pre_test_B.reset_index(drop=True)\n",
    "\n",
    "X_train_M = X_train_M.reset_index(drop=True)\n",
    "X_pre_test_M = X_pre_test_M.reset_index(drop=True)\n",
    "y_train_M = y_train_M.reset_index(drop=True)\n",
    "y_pre_test_M = y_pre_test_M.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Analysing and Visualising the dataset\n",
    "\n",
    "#### 2.2.1 Visualising the images\n",
    "\n",
    "\n",
    "The dataset has 62210 images, with around 964 features. To visualise the images, I sliced the first\n",
    "900 columns to get the Histogram of Orientated Gradient (HOG) features and picked an image from each class found in the Y_train datasets\n",
    "\n",
    "Figure 1 below, showes the images taken from the binary dataset, where classes are either background and seal. \n",
    "Figure 2, shows images from the multi dataset, where images are background, dead pup, whitecoat, moulted pup, juvenile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_classes = ['background','seal']\n",
    "mult_classes = ['background','dead pup', 'whitecoat', 'moulted pup', 'juvenile']\n",
    "\n",
    "def get_rows_and_titles(classes, dataset, num):\n",
    "    \"\"\"\n",
    "    Method for getting the row numbers and titles for each class\n",
    "    \"\"\"\n",
    "    rows = np.array([])\n",
    "    titles = np.array([])\n",
    "    for i in classes:\n",
    "        filt = dataset.iloc[:,0] == i\n",
    "        filtered_data = dataset[filt].head(num)\n",
    "        rows = np.concatenate([rows, filtered_data.index.values])\n",
    "        titles = np.concatenate([titles, array(filtered_data.iloc[:,0])])\n",
    "    \n",
    "    return rows.astype(int), titles  \n",
    "      \n",
    "        \n",
    "bin_rows, bin_titles = get_rows_and_titles(bin_classes, y_train_B,1)\n",
    "mult_rows, mult_titles = get_rows_and_titles(mult_classes, y_train_M,1)\n",
    "\n",
    "bin_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes in the  binary dataset\n",
    "no_rows_bin  = 2\n",
    "no_rows_mult = 2\n",
    "def print_images(no_rows,row_numbers, titles, dataset, width, height): \n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    for i in range(1, len(row_numbers) +1): \n",
    "        output = array(dataset.iloc[row_numbers[i-1], :900])\n",
    "        output_image = output.reshape(30, 30)\n",
    "        fig.add_subplot(no_rows, 5, i)\n",
    "        plt.imshow(output_image, cmap = mpl.cm.binary, interpolation=\"nearest\")\n",
    "        plt.title(titles[i-1])\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "print_images(no_rows_bin, bin_rows, bin_titles, X_train_B,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_images(no_rows_mult, mult_rows, mult_titles, X_train_M,10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both datasets, we can see that the \"backgound\" classes appears to have the least amount of dark pixelation compared to the others. \n",
    "In the mult-classification dataset, the whitecoat image appears to be the most distinct\n",
    "\n",
    "#### 2.2.2 Visualising the class distribution in the data\n",
    "\n",
    "To understand the distribution of the classes in both the binary and multi datasets, I visualised the frequency distribution of the classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for visualisation\n",
    "bin_freq= y_train_B.iloc[:,0].value_counts()\n",
    "mult_freq= y_train_M.iloc[:,0].value_counts()\n",
    "\n",
    "mult_classes = array(mult_freq.keys())\n",
    "mult_data = mult_freq.values\n",
    "bin_classes = array(bin_freq.keys())\n",
    "bin_data = bin_freq.values\n",
    "\n",
    "classes = [mult_classes, bin_classes]\n",
    "mult_and_bin_freq = [mult_data, bin_data]\n",
    "titles = [\"Multi\",\"Binary\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_multi.iloc[:,0].value_counts().plot(kind='barh') #horizontal bar plot\n",
    "\n",
    "def get_pct(array, value):\n",
    "    pct = value/array.sum()\n",
    "    pct_string =  \" ({:.2%})\".format(pct)\n",
    "    return pct_string\n",
    "#\n",
    "def plot_class_dist(data,classes, title):\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))    \n",
    "    width = 0.75 # the width of the bars \n",
    "    ind = np.arange(len(data))  # the x locations for the groups\n",
    "    ax.barh(ind, data, width, color=\"blue\")\n",
    "    ax.set_yticks(ind+width/2)\n",
    "    ax.set_yticklabels(classes, minor=False)\n",
    "    plt.title(\"Class Frequency Distribution of the \" + title +  \" Dataset\")\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Classes')\n",
    "    for i, v in enumerate(data):\n",
    "        ax.text(v + 3, i + .25, str(v) + get_pct(data,v), color='green', fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "for i in range(2):\n",
    "    plot_class_dist(mult_and_bin_freq[i],classes[i],titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that both datasets are heavily skewed towards the background class, accounting for 87.5% of the dataset. Given this large proportion of the background class in the dataset,\n",
    "it may be no suprise that the classification algorithm is able to classify the background class very well. On the other hand, the juvenile and moulted pup classes\n",
    "account for 0.40% and 0.45% respectively. Given the skewed data set it will make sense to stratify the training sets to keep a consistent distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Visualising Variance Using PCA and t-SNE\n",
    "\n",
    "**PCA**\n",
    "\n",
    "PCA is a popular technique for reducing the number of dimensions in a dataset whilst retaining most information. PCA preserves variances by identifying the axis that accounts for the largest amount of variance in the training set (hands on). It then finds a 2nd axis which is orthogonal to the first one, that accounts for the largest amount of remaining variance. In higher dimensions, PCA would find a 3rd axis and then an ith axis, based on the number of dimensions in the dataset. The vector that defines each axis is called the the Priniciapal component. For visualising purposes, I reduced the dimensions down to 3 dimensions and plotted the 1st and 2nd Principal componetns against each other. (See figure 5). Using the explained \n",
    "variance ratio, I found that 35.6% of the dataset's variance lied in the first axis, 20.1% in the 2nd axis and 14% in the 3rd axis. Thus the first three components accounted for 69.7% of the entire dataset. \n",
    "\n",
    "https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n",
    "\n",
    "\n",
    "We can see from Figure 3, that the data relating to the seal has the highest variance as it dominates the X axis of PCA 1. The data relating to background, appears to have a lower variance as it appears for the highest PCA values for PCA 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**t-SNE**\n",
    "\n",
    "original paper = http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf\n",
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) is another technique for dimensionality reduction and is particularly well suited for the visualization of high-dimensional datasets. Contrary to PCA it is not a mathematical technique but a probablistic one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative variance explained by 50 principal components: 0.6729516004620906\n",
      "[t-SNE] Computing 121 nearest neighbors...\n",
      "[t-SNE] Indexed 40000 samples in 0.677s...\n",
      "[t-SNE] Computed neighbors for 40000 samples in 498.312s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 19000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 20000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 21000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 22000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 23000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 24000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 25000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 26000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 27000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 28000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 29000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 30000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 31000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 32000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 33000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 34000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 35000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 36000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 37000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 38000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 39000 / 40000\n",
      "[t-SNE] Computed conditional probabilities for sample 40000 / 40000\n",
      "[t-SNE] Mean sigma: 4.501511\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 105.171745\n",
      "[t-SNE] KL divergence after 300 iterations: 5.236198\n",
      "t-SNE done! Time elapsed: 565.3140389919281 seconds\n",
      "Cumulative variance explained by 50 principal components: 0.6723768202265187\n",
      "[t-SNE] Computing 121 nearest neighbors...\n",
      "[t-SNE] Indexed 40000 samples in 0.770s...\n"
     ]
    }
   ],
   "source": [
    "# scale binary dataset\n",
    "std_B = StandardScaler().fit(X_train_B)\n",
    "X_train_B_std = std_B.transform(X_train_B)\n",
    "\n",
    "# scale multi dataset\n",
    "std_M = StandardScaler().fit(X_train_M)\n",
    "X_train_M_std = std_M.transform(X_train_M)\n",
    "\n",
    "# data lists\n",
    "X_train_std_list = [X_train_M_std, X_train_B_std]\n",
    "Y_train_list = [y_train_M,y_train_B]\n",
    "no_colors = [5,2]\n",
    "   \n",
    "def plot_pca(X_train_list,y_train_list, no_colors, titles):\n",
    "    plt.figure(figsize=(16,7))\n",
    "    for j in range(2):\n",
    "        \n",
    "        # prepare data for visualisation\n",
    "        y = array(y_train_list[j].iloc[:,0])\n",
    "        feat_cols = ['pixel'+str(i) for i in range(X_train_B.shape[1])]\n",
    "        df = pd.DataFrame(array(X_train_list[j]), columns=feat_cols)\n",
    "        df['y'] = y\n",
    "        df['label'] = df['y'].apply(lambda i: str(i))\n",
    "\n",
    "        X, y = None, None\n",
    "\n",
    "        # add randome permuation for reproducability of the results\n",
    "        np.random.seed(42)\n",
    "        rndperm = np.random.permutation(X_train_list[j].shape[0])\n",
    "\n",
    "        # Set PCA components\n",
    "        N = 60000\n",
    "        df_subset = df.loc[rndperm[:N],:].copy()\n",
    "        data_subset = df_subset[feat_cols].values\n",
    "        pca = PCA(n_components=3)\n",
    "        pca_result = pca.fit_transform(data_subset)\n",
    "        df_subset['pca-one'] = pca_result[:,0]\n",
    "        df_subset['pca-two'] = pca_result[:,1] \n",
    "        df_subset['pca-three'] = pca_result[:,2]\n",
    "        print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "        \n",
    "        # plot PCA components \n",
    "        \n",
    "        ax1 = plt.subplot(1, 2, j+1)\n",
    "        sb.scatterplot(\n",
    "            x=\"pca-one\", y=\"pca-two\",\n",
    "            hue=\"y\",\n",
    "            palette=sb.color_palette(\"hls\", no_colors[j]),\n",
    "            data=df_subset,\n",
    "            legend=\"full\",\n",
    "            alpha=0.3,\n",
    "            ax=ax1\n",
    "        )\n",
    "        ax1.set_title(titles[j] + \" dataset\")\n",
    "\n",
    "def plot_tsne(X_train_list,y_train_list, no_colors, titles):\n",
    "    plt.figure(figsize=(16,7))\n",
    "    for j in range(2):\n",
    "        \n",
    "        # prepare data for visualisation\n",
    "        y = array(y_train_list[j].iloc[:,0])\n",
    "        feat_cols = ['pixel'+str(i) for i in range(X_train_B.shape[1])]\n",
    "        df = pd.DataFrame(array(X_train_list[j]), columns=feat_cols)\n",
    "        df['y'] = y\n",
    "        df['label'] = df['y'].apply(lambda i: str(i))\n",
    "\n",
    "        X, y = None, None\n",
    "\n",
    "        # add randome permuation for reproducability of the results\n",
    "        np.random.seed(42)\n",
    "        rndperm = np.random.permutation(X_train_list[j].shape[0])\n",
    "\n",
    "        # Set PCA components\n",
    "        N = 40000\n",
    "        df_subset = df.loc[rndperm[:N],:].copy()\n",
    "        data_subset = df_subset[feat_cols].values\n",
    "        pca = PCA(n_components=100)\n",
    "        pca_result = pca.fit_transform(data_subset)\n",
    "        print('Cumulative variance explained by 100 principal components: {}'.format(np.sum(pca.explained_variance_ratio_)))\n",
    "        \n",
    "        #Set TSNE components\n",
    "        time_start = time.time()\n",
    "        tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "        tsne_results = tsne.fit_transform(pca_result)\n",
    "        print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "        df_subset['tsne-2d-one'] = tsne_results[:,0]\n",
    "        df_subset['tsne-2d-two'] = tsne_results[:,1]\n",
    "\n",
    "        # plot PCA components \n",
    "        \n",
    "        ax1 = plt.subplot(1, 2, j+1)\n",
    "        sb.scatterplot(\n",
    "            x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "            hue=\"y\",\n",
    "            palette=sb.color_palette(\"hls\", no_colors[j]),\n",
    "            data=df_subset,\n",
    "            legend=\"full\",\n",
    "            alpha=0.3,\n",
    "            ax=ax1\n",
    "        )\n",
    "        ax1.set_title(titles[j] + \" dataset\")\n",
    "    \n",
    "    \n",
    "    \n",
    "# for i in range (2):\n",
    "plot_tsne(X_train_std_list, Y_train_list, no_colors, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Preparing the inputs and choosing suitable features (TODO)\n",
    "\n",
    "- Discuss about the features and their properties i.e HOG features, their importance and everything else\n",
    "- Understand the correlation of the features with one another\n",
    "\n",
    "\n",
    "To have a better understanding of how the features related with one .\n",
    "\n",
    "The dataset is imbalanced - https://www.aaai.org/Papers/Workshops/2000/WS-00-05/WS00-05-001.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Choosing Suitable features \n",
    "\n",
    "Since the data is split up into three different groups, I decided to test different feature subsets using a baseline logistic regression, in order to choose a \n",
    "specific subset for further training and optimising. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary dataset feature subsets\n",
    "X_train_B_HoG = X_train_B.iloc[:, :900]\n",
    "X_train_B_normal = X_train_B.iloc[:, 900:916]\n",
    "X_train_B_colour = X_train_B.iloc[:, 916:964]\n",
    "X_train_B_list=[X_train_B, X_train_B_HoG, X_train_B_normal, X_train_B_colour]\n",
    "\n",
    "# Binary dataset feature subsets\n",
    "X_train_M_HoG = X_train_B.iloc[:, :900]\n",
    "X_train_M_normal = X_train_B.iloc[:, 900:916]\n",
    "X_train_M_colour = X_train_B.iloc[:, 916:964]\n",
    "X_train_M_list=[X_train_M, X_train_M_HoG, X_train_M_normal, X_train_M_colour]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Scaling\n",
    "\n",
    "The paper mentioned that scaling was important for getting good results as normalisation helps to remove the effect of local lights differences. \n",
    "\n",
    "http://www.geocities.ws/talh_davidc/#cst_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale binary dataset - both training and test sets\n",
    "scaler_B = StandardScaler()\n",
    "X_train_B_HoG = scaler_B.fit_transform(X_train_B_HoG)\n",
    "X_pre_test_B_HoG = pd.DataFrame(scaler_B.transform(X_pre_test_B.iloc[:, :900]))\n",
    "X_test_binary_HoG = pd.DataFrame(scaler_B.transform(X_test_binary.iloc[:, :900]))\n",
    "\n",
    "# scale multi dataset - both training and test sets\n",
    "scaler_M = StandardScaler()\n",
    "X_train_M_HoG = pd.DataFrame(scaler_M.fit_transform(X_train_M_HoG))\n",
    "X_pre_test_M_HoG = pd.DataFrame(scaler_M.transform(X_pre_test_M.iloc[:, :900]))\n",
    "X_test_multi_HoG = pd.DataFrame(scaler_M.transform(X_test_multi.iloc[:, :900]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Dimensionality reduction with PCA\n",
    "\n",
    "Earlier dimensionality reduction was used for visualisation, now it will be used for training and evaluation purposes. As opposed to only used 2 principal components\n",
    ", the aim would be to find the optimal amount of principal components so as to reduce the dimensions while having a high amount of variance in the data > 95%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train_B_HoG)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.99) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(cumsum, linewidth=3)\n",
    "plt.axis([0, 964, 0, 1])\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.plot([d, d], [0, 0.99], \"k:\")\n",
    "plt.plot([0, d], [0.99, 0.99], \"k:\")\n",
    "plt.plot(d, 0.99, \"ko\")\n",
    "plt.grid(True)\n",
    "plt.savefig('figures/explained variance vs dimension.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use minimum number of dimensions required to preserve 99% of the variance\n",
    "pca = PCA(n_components=444)\n",
    "X_train_B_PCA = pca.fit_transform(X_train_B_HoG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Resampling\n",
    "\n",
    "Study on why PCA should be done before - https://arxiv.org/ftp/arxiv/papers/1403/1403.1949.pdf\n",
    "\n",
    "To address the issue of having an unbalanced dataset, this study investigated resampling techniques such as oversampling and undersampling\n",
    "\n",
    "- Oversampling: Oversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you don’t have a ton of data to work with.\n",
    "- Undersampling: is a method of reducing copies of a majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_bin(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Oversamples the minority class of a binary dataset and\n",
    "    return: X_train_ov, y_train_ov\n",
    "    \"\"\"\n",
    "    # concatenate our training data back together\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    y_train = y_train.rename(columns={0: \"classes\"})\n",
    "    X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "    # separate minority and majority classes\n",
    "    not_seal_bool = X['classes']=='background'\n",
    "    not_seal = X[not_seal_bool]\n",
    "    seal_bool = X['classes']=='seal'\n",
    "    seal = X[seal_bool]\n",
    "\n",
    "    # upsample minority\n",
    "    seal_upsampled = resample(seal,\n",
    "                              replace=True, # sample with replacement\n",
    "                              n_samples=len(not_seal), # match number in majority class\n",
    "                              random_state=27) # reproducible results\n",
    "\n",
    "    # combine majority and upsampled minority\n",
    "    upsampled = pd.concat([not_seal, seal_upsampled])\n",
    "    y_train_ov = upsampled['classes']\n",
    "    X_train_ov = upsampled.drop(columns='classes', axis=1)\n",
    "    oversampled_data = [X_train_ov, y_train_ov]\n",
    "    return oversampled_data\n",
    "   \n",
    "def undersample_bin(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Undersamples the minority class of a binary dataset and\n",
    "    return: X_train_ov, y_train_ov\n",
    "    \"\"\"\n",
    "    # concatenate our training data back together\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    y_train = y_train.rename(columns={0: \"classes\"})\n",
    "    X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "    # separate minority and majority classes\n",
    "    not_seal_bool = X['classes']=='background'\n",
    "    not_seal = X[not_seal_bool]\n",
    "    seal_bool = X['classes']=='seal'\n",
    "    seal = X[seal_bool]\n",
    "\n",
    "    # downsample majority\n",
    "    not_seal_downsampled = resample(not_seal,\n",
    "                                    replace = False, # sample without replacement\n",
    "                                    n_samples = len(seal), # match minority n\n",
    "                                    random_state = 27) # reproducible results\n",
    "\n",
    "    # combine minority and downsampled majority\n",
    "    downsampled = pd.concat([not_seal_downsampled, seal])\n",
    "    y_train_down = downsampled['classes']\n",
    "    X_train_down = downsampled.drop(columns='classes', axis=1)\n",
    "    undersampled_data = [X_train_down, y_train_down]\n",
    "    return undersampled_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating synthetic data\n",
    "\n",
    "Paper on the combination of PCA and SMOTE: https://arxiv.org/ftp/arxiv/papers/1403/1403.1949.pdf\n",
    "\n",
    "A technique similar to upsampling is to create synthetic samples. Here we will use imblearn’s SMOTE or Synthetic Minority Oversampling Technique. \n",
    "SMOTE uses a nearest neighbors algorithm to generate new and synthetic data we can use for training our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE\n",
    "\n",
    "def smote_bin(X_train, y_train):\n",
    "    sm = SMOTE(random_state=27)\n",
    "    X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "    smote_data = [X_train, y_train]\n",
    "    return smote_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_data = {'SMOTE':smote_bin(X_train_B_HoG,y_train_B), \n",
    "                   'Undersampled':undersample_bin(X_train_B_HoG, y_train_B),\n",
    "                  'Oversampled': oversample_bin(X_train_B_HoG, y_train_B),\n",
    "                    'Original': [X_train_B_HoG,y_train_B]}\n",
    "\n",
    "resampled_PCA =  {'SMOTE_PCA':smote_bin(X_train_B_PCA,y_train_B), \n",
    "                   'Undersampled_PCA':undersample_bin(X_train_B_PCA, y_train_B),\n",
    "                  'Oversampled_PCA': oversample_bin(X_train_B_PCA, y_train_B),\n",
    "                    'Original_PCA': [X_train_B_PCA,y_train_B]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Training The Model \n",
    "\n",
    "### 2.4.1 Models used \n",
    "\n",
    "- Logistic regression\n",
    "- SVM\n",
    "- MLP\n",
    "- Random Forrest - imbalanced dataset with RF https://ieeexplore.ieee.org/abstract/document/4410397\n",
    "\n",
    "### 2.4.2 Out of the box classifier\n",
    "\n",
    "The first models that I will use are logistic regression and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the logistic regression\n",
    "\n",
    "def run_rf_cv(X, Y):\n",
    "    \"\"\"\n",
    "    Runs the Random Forest classifier using 3 fold cross validation. Trains the model \n",
    "    and scores it on the given dataset, providing results for accuracy, precision, recall, f1 score\n",
    "    and a confusion matrix\n",
    "    :return: log\n",
    "    \"\"\"\"\"\n",
    "    Y = Y.values.ravel()\n",
    "    results = {}\n",
    "    rf = RandomForestClassifier(max_depth=10, n_estimators=10, random_state=0, n_jobs=-1).fit(X, Y)\n",
    "    scores = cross_val_score(rf, X, Y, cv=3, scoring=\"accuracy\")\n",
    "    avg_accuracy = sum(scores)/len(scores)\n",
    "    Y_pred = cross_val_predict(rf, X, Y, cv=3)\n",
    "    cm = confusion_matrix(Y, Y_pred)   \n",
    "    results[\"Accuracy\"] = sum(scores)/len(scores)\n",
    "    results[\"Precision\"] = precision_score(Y, Y_pred, pos_label='seal')\n",
    "    results[\"Recall\"] = recall_score(Y, Y_pred, pos_label='seal')\n",
    "    results[\"F1 Score\"] = f1_score(Y, Y_pred, pos_label='seal')\n",
    "    print(\"Random Forest Performance Results: \\n\" + str(results))\n",
    "    print(cm) \n",
    "    return rf, results\n",
    "\n",
    "def run_log_cv(X, Y,dataset_name):\n",
    "    \"\"\"\n",
    "    Runs the Logistic regression classifier using 3 fold cross validation. Trains the model \n",
    "    and scores it on the given dataset, providing results for accuracy, precision, recall, f1 score\n",
    "    and a confusion matrix\n",
    "    :return: log\n",
    "    \"\"\"\"\"\n",
    "    Y = Y.values.ravel()\n",
    "    results = {}\n",
    "    log = LogisticRegression(random_state=0).fit(X, Y)\n",
    "    scores = cross_val_score(log, X, Y, cv=3, scoring=\"accuracy\")\n",
    "    avg_accuracy = sum(scores)/len(scores)\n",
    "    Y_pred = cross_val_predict(log, X, Y, cv=3)\n",
    "    cm = confusion_matrix(Y, Y_pred)   \n",
    "    results[\"Accuracy\"] = sum(scores)/len(scores)\n",
    "    results[\"Precision\"] = precision_score(Y, Y_pred, pos_label='seal')\n",
    "    results[\"Recall\"] = recall_score(Y, Y_pred, pos_label='seal')\n",
    "    results[\"F1 Score\"] = f1_score(Y, Y_pred, pos_label='seal')\n",
    "    print(\"Logistic Regression Performance Results on the {0} dataset: \\n\".format(dataset_name) + str(results))\n",
    "    print(cm) \n",
    "    return log, results\n",
    "\n",
    "def run_log(X, Y, dataset_name):\n",
    "    \"\"\"\n",
    "    Runs the Logistic regression classifier. Trains the model \n",
    "    and scores it on the given dataset, providing results for accuracy, precision, recall, f1 score\n",
    "    and a confusion matrix\n",
    "    :return: log\n",
    "    \"\"\"\"\"\n",
    "    Y = Y.values.ravel()\n",
    "    results = {}\n",
    "    log = LogisticRegression(random_state=0).fit(X, Y)\n",
    "    Y_pred = log.predict(X)\n",
    "    cm = confusion_matrix(Y, Y_pred)   \n",
    "    results[\"Accuracy\"] = precision_score(Y, Y_pred, pos_label='seal')\n",
    "    results[\"Precision\"] = precision_score(Y, Y_pred, pos_label='seal')\n",
    "    results[\"Recall\"] = recall_score(Y, Y_pred, pos_label='seal')\n",
    "    results[\"F1 Score\"] = f1_score(Y, Y_pred, pos_label='seal')\n",
    "    print(\"Logistic Regression Performance Results on the {0} dataset: \\n\".format(dataset_name) + str(results))\n",
    "    print(cm) \n",
    "    return log, results\n",
    "\n",
    "\n",
    "def run_svc(X, Y, dataset_name):\n",
    "    \"\"\"\n",
    "    Runs the Logistic regression classifier. Trains the model \n",
    "    and scores it on the given dataset, providing results for accuracy, precision, recall, f1 score\n",
    "    and a confusion matrix\n",
    "    :return: log\n",
    "    \"\"\"\"\"\n",
    "    Y = Y.values.ravel()\n",
    "    results = {}\n",
    "    svc = SVC(kernel='poly',degree=2, max_iter=10000, verbose=True).fit(X, Y)\n",
    "    Y_pred = svc.predict(X)\n",
    "    cm = confusion_matrix(Y, Y_pred)\n",
    "    results[\"Accuracy\"] = precision_score(Y, Y_pred, pos_label='seal')\n",
    "    results[\"Precision\"] = precision_score(Y, Y_pred, pos_label='seal')\n",
    "    results[\"Recall\"] = recall_score(Y, Y_pred, pos_label='seal')\n",
    "    results[\"F1 Score\"] = f1_score(Y, Y_pred, pos_label='seal')\n",
    "    print(\"SVC Performance Results on the {0} dataset: \\n\".format(dataset_name) + str(results))\n",
    "    print(cm) \n",
    "    return svc, results\n",
    "\n",
    "# Y = X_train_multi.values.ravel()\n",
    "# run_svc(X_train_B,Y)\n",
    "# log = LogisticRegression(random_state=0).fit(X_train_B,Y)\n",
    "# print(\"Logistic Regression Test Score: {0}\".format(log.score(X_train_B,Y)))\n",
    "\n",
    "time_start = time.time()\n",
    "for key in resampled_PCA:\n",
    "    run_svc(resampled_PCA[key][0],resampled_PCA[key][1],key)\n",
    "    print('Prediction done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "for key in resampled_data:\n",
    "    run_svc(resampled_data[key][0],resampled_data[key][1],key)\n",
    "    print('Prediction done! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
