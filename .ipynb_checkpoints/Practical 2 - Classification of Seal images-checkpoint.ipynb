{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib as mp\n",
    "import time\n",
    "import seaborn as sb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections of the report \n",
    "\n",
    "-\tAbstract\n",
    "-\tIntroduction\n",
    "-\tMethods\n",
    "    -\tCleaning the data and creating new input features\n",
    "    - Analysing and visualising the data\n",
    "    - Preparing the inputs and choosing suitable features\n",
    "    - Selecting and training a model\n",
    "- Initial model selection\n",
    "- Training the model\n",
    "- K fold cross validation\n",
    "- Fine-tuning the models\n",
    "-\tEvaluation\n",
    "-\tDiscussion \n",
    "-\tConclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This study will be taking a data driven approach towards classifying images of seals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Method\n",
    "\n",
    "### 2. 1 Loading and cleaning the data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "X_train_bin = pd.read_csv('dataset/binary/X_train.csv')\n",
    "Y_train_bin = pd.read_csv('dataset/binary/Y_train.csv')\n",
    "X_test_bin = pd.read_csv('dataset/binary/X_test.csv')\n",
    "X_train_mult = pd.read_csv('dataset/multi/X_train.csv')\n",
    "Y_train_mult = pd.read_csv('dataset/multi/Y_train.csv')\n",
    "X_test_mult = pd.read_csv('dataset/multi/X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62209 entries, 0 to 62208\n",
      "Columns: 964 entries, 1.420994926690040683e-01 to 5.000000000000000000e+00.1\n",
      "dtypes: float64(964)\n",
      "memory usage: 457.5 MB\n"
     ]
    }
   ],
   "source": [
    "X_train_mult.info(  null_counts=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon looking at that dataset, we can see that each column has an equal amount of 62209 non-null values, indicating that there are no instances of missing data. Now let's check for any duplicate entries in all the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "62207\n",
      "0\n",
      "0\n",
      "62204\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# remove duplicate rows\n",
    "datasets = [X_train_bin,Y_train_bin, X_test_bin, X_train_mult, Y_train_mult, X_test_mult]\n",
    "\n",
    "for data in datasets:\n",
    "    duplicate_rows = data[data.duplicated()]\n",
    "    print(len(duplicate_rows))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon checking the dataset, there were no null values, no duplicate and all the data types were for the dtype float64. Nothing needed to change so will proceed to visualising the dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
